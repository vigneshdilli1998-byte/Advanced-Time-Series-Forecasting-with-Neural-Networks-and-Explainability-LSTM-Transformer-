{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtlrCDXxzlWd"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "from typing import Tuple, Dict, Any\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import trange, tqdm\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import statsmodels.api as sm\n",
        "import shap\n",
        "from captum.attr import IntegratedGradients\n",
        "\n",
        "# -----------------------------\n",
        "# 1) DATA GENERATION\n",
        "# -----------------------------\n",
        "def generate_multiseasonal_multivariate(n_samples: int = 3000, random_seed: int = 42) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Produce a multivariate time series with:\n",
        "    - trend (nonstationarity)\n",
        "    - two seasonalities (daily-like + weekly-like)\n",
        "    - multiplicative seasonal effects\n",
        "    - occasional regime shifts (structural breaks)\n",
        "    - multiple channels/features with coupling\n",
        "    Returns a DataFrame with columns: ['t','feat_0','feat_1','feat_2']\n",
        "    \"\"\"\n",
        "    np.random.seed(random_seed)\n",
        "    t = np.arange(n_samples)\n",
        "    # base trend (non-stationary)\n",
        "    trend = 0.001 * (t**1.2)  # slowly accelerating trend\n",
        "\n",
        "    # seasonality A: period 24\n",
        "    period_a = 24\n",
        "    season_a = 1.5 * np.sin(2 * np.pi * t / period_a) + 0.3 * np.sin(2 * np.pi * t / (period_a/2))\n",
        "\n",
        "    # seasonality B: period 168 (weekly)\n",
        "    period_b = 168\n",
        "    season_b = 2.0 * np.sin(2 * np.pi * t / period_b) + 0.5 * np.sin(2 * np.pi * t / (period_b/3))\n",
        "\n",
        "    # multiplicative interaction and noise\n",
        "    noise = 0.5 * np.random.randn(n_samples)\n",
        "\n",
        "    # regime shifts: add jump at two points\n",
        "    shift = np.zeros(n_samples)\n",
        "    shift[n_samples//3:] += 1.5\n",
        "    shift[2*n_samples//3:] -= 0.8\n",
        "\n",
        "    # Construct three features with coupling + nonlinear transformations\n",
        "    base = (trend + season_a + season_b + shift)\n",
        "    feat0 = base + 0.3 * np.sin(0.05 * t) + 0.2 * noise\n",
        "    feat1 = 0.5 * base + 0.2 * np.cos(0.02 * t) + 0.3 * np.random.randn(n_samples)\n",
        "    # nonlinear coupling of past values to produce more complexity\n",
        "    feat2 = 0.3 * np.roll(base, 1) + 0.7 * np.tanh(base) + 0.25 * np.random.randn(n_samples)\n",
        "    # first element fix after roll\n",
        "    feat2[0] = feat0[0] * 0.2 + 0.1 * np.random.randn()\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        't': t,\n",
        "        'feat_0': feat0,\n",
        "        'feat_1': feat1,\n",
        "        'feat_2': feat2\n",
        "    })\n",
        "    return df\n",
        "\n",
        "# -----------------------------\n",
        "# 2) PREPROCESSING: SCALER + WINDOWING\n",
        "# -----------------------------\n",
        "class TimeSeriesWindowDataset(Dataset):\n",
        "    def __init__(self, data: np.ndarray, input_width: int, output_width: int, stride: int = 1):\n",
        "        \"\"\"\n",
        "        data: shape (n_samples, n_features)\n",
        "        Returns windows for multi-step forecasting:\n",
        "         - X: shape (num_windows, input_width, n_features)\n",
        "         - y: shape (num_windows, output_width, target_dim)  (we'll predict first feature by default)\n",
        "        \"\"\"\n",
        "        self.data = data.astype(np.float32)\n",
        "        self.input_width = input_width\n",
        "        self.output_width = output_width\n",
        "        self.stride = stride\n",
        "        self.n_samples, self.n_features = data.shape\n",
        "        self.indices = []\n",
        "        # create indices\n",
        "        max_start = self.n_samples - (input_width + output_width) + 1\n",
        "        for s in range(0, max_start, stride):\n",
        "            self.indices.append(s)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        s = self.indices[idx]\n",
        "        x = self.data[s: s + self.input_width]  # shape (input_width, n_features)\n",
        "        y = self.data[s + self.input_width: s + self.input_width + self.output_width, 0:1]  # predict feat_0\n",
        "        return x, y\n",
        "\n",
        "# -----------------------------\n",
        "# 3) MODEL: PyTorch LSTM\n",
        "# -----------------------------\n",
        "class LSTMForecast(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_dim: int, num_layers: int, output_width: int, dropout: float = 0.0):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
        "        # map last hidden state to multi-step outputs\n",
        "        self.fc = nn.Linear(hidden_dim, output_width)  # output_width for scalar target per step\n",
        "        # we will output a vector of length output_width; interpret as next output_width steps for feature 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, input_dim)\n",
        "        out, (hn, cn) = self.lstm(x)  # out: (batch, seq_len, hidden)\n",
        "        # take last timestep hidden state\n",
        "        last = out[:, -1, :]  # (batch, hidden_dim)\n",
        "        out = self.fc(last)   # (batch, output_width)\n",
        "        # reshape to (batch, output_width, 1) for compatibility\n",
        "        return out.unsqueeze(-1)\n",
        "\n",
        "# -----------------------------\n",
        "# 4) TRAIN / EVAL UTIL\n",
        "# -----------------------------\n",
        "def train_model(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader,\n",
        "                n_epochs: int, lr: float, device: torch.device, verbose=True) -> Dict[str, Any]:\n",
        "    model.to(device)\n",
        "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "    history = {'train_loss': [], 'val_loss': []}\n",
        "    best_val = float('inf')\n",
        "    best_state = None\n",
        "\n",
        "    for epoch in range(1, n_epochs+1):\n",
        "        model.train()\n",
        "        running = 0.0\n",
        "        for xb, yb in train_loader:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            optim.zero_grad()\n",
        "            pred = model(xb)\n",
        "            loss = criterion(pred, yb)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            running += loss.item() * xb.size(0)\n",
        "        train_loss = running / len(train_loader.dataset)\n",
        "        # validation\n",
        "        model.eval()\n",
        "        running_v = 0.0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb = xb.to(device); yb = yb.to(device)\n",
        "                pred = model(xb)\n",
        "                running_v += nn.MSELoss(reduction='sum')(pred, yb).item()\n",
        "        val_loss = running_v / len(val_loader.dataset)\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        if verbose and epoch % max(1, n_epochs//10) == 0:\n",
        "            print(f\"Epoch {epoch}/{n_epochs}  train_mse={train_loss:.6f}  val_mse={val_loss:.6f}\")\n",
        "        if val_loss < best_val:\n",
        "            best_val = val_loss\n",
        "            best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "    return {'model': model, 'history': history, 'best_val_mse': best_val}\n",
        "\n",
        "def predict_numpy(model: nn.Module, X: np.ndarray, device: torch.device) -> np.ndarray:\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        tX = torch.from_numpy(X.astype(np.float32)).to(device)\n",
        "        y = model(tX).cpu().numpy()  # (batch, output_width, 1)\n",
        "    return y.squeeze(-1)\n",
        "\n",
        "# -----------------------------\n",
        "# 5) METRICS\n",
        "# -----------------------------\n",
        "def rmse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
        "    return float(np.sqrt(mean_squared_error(y_true.flatten(), y_pred.flatten())))\n",
        "\n",
        "def mape(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
        "    denom = np.where(np.abs(y_true) < 1e-6, 1e-6, np.abs(y_true))\n",
        "    return float(np.mean(np.abs((y_true - y_pred) / denom)) * 100.0)\n",
        "\n",
        "# -----------------------------\n",
        "# 6) STATISTICAL BASELINE: SARIMA\n",
        "# -----------------------------\n",
        "def sarima_forecast(train_series: pd.Series, test_len: int, order=(1,1,1), seasonal_order=(1,1,1,24)):\n",
        "    \"\"\"\n",
        "    Fit SARIMAX (simple) and forecast test_len steps ahead.\n",
        "    Returns forecast array of length test_len.\n",
        "    \"\"\"\n",
        "    # statsmodels expects endog as Series\n",
        "    model = sm.tsa.statespace.SARIMAX(train_series, order=order, seasonal_order=seasonal_order, enforce_stationarity=False, enforce_invertibility=False)\n",
        "    res = model.fit(disp=False)\n",
        "    pred = res.get_forecast(steps=test_len)\n",
        "    mean = pred.predicted_mean.values\n",
        "    return mean, res\n",
        "\n",
        "# -----------------------------\n",
        "# 7) MAIN PIPELINE\n",
        "# -----------------------------\n",
        "def main_pipeline():\n",
        "    seed = 123\n",
        "    np.random.seed(seed); torch.manual_seed(seed); random.seed(seed)\n",
        "    # 1) generate\n",
        "    n = 3000  # >= 1000 as required\n",
        "    df = generate_multiseasonal_multivariate(n_samples=n, random_seed=seed)\n",
        "    print(\"Generated data sample:\\n\", df.head())\n",
        "\n",
        "    # Split: train/val/test (60/20/20)\n",
        "    train_end = int(0.6 * n)\n",
        "    val_end = int(0.8 * n)\n",
        "    train_df = df.iloc[:train_end].reset_index(drop=True)\n",
        "    val_df = df.iloc[train_end:val_end].reset_index(drop=True)\n",
        "    test_df = df.iloc[val_end:].reset_index(drop=True)\n",
        "\n",
        "    # Save CSVs\n",
        "    os.makedirs('output', exist_ok=True)\n",
        "    df.to_csv('output/full_series.csv', index=False)\n",
        "    train_df.to_csv('output/train.csv', index=False)\n",
        "    val_df.to_csv('output/val.csv', index=False)\n",
        "    test_df.to_csv('output/test.csv', index=False)\n",
        "\n",
        "    # 2) scale\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(train_df[['feat_0', 'feat_1', 'feat_2']].values)\n",
        "    train_scaled = scaler.transform(train_df[['feat_0','feat_1','feat_2']])\n",
        "    val_scaled = scaler.transform(val_df[['feat_0','feat_1','feat_2']])\n",
        "    test_scaled = scaler.transform(test_df[['feat_0','feat_1','feat_2']])\n",
        "\n",
        "    # 3) create window datasets\n",
        "    input_width = 168  # use one weekly range as input\n",
        "    output_width = 24  # predict next day (multi-step)\n",
        "    batch_size = 64\n",
        "\n",
        "    train_ds = TimeSeriesWindowDataset(train_scaled, input_width, output_width, stride=3)\n",
        "    val_ds = TimeSeriesWindowDataset(np.vstack([train_scaled[-input_width:], val_scaled]), input_width, output_width, stride=1)  # val windows crossing train/val boundary\n",
        "    test_ds = TimeSeriesWindowDataset(np.vstack([val_scaled[-input_width:], test_scaled]), input_width, output_width, stride=1)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # 4) hyperparameter search (simple grid)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    hyperparams = [\n",
        "        {'hidden_dim': 64, 'num_layers': 1, 'lr': 1e-3, 'dropout': 0.0, 'n_epochs': 20},\n",
        "        {'hidden_dim': 128, 'num_layers': 2, 'lr': 5e-4, 'dropout': 0.1, 'n_epochs': 30},\n",
        "    ]\n",
        "    best = None\n",
        "    best_val = float('inf')\n",
        "    for hp in hyperparams:\n",
        "        print(\"Training with hp:\", hp)\n",
        "        model = LSTMForecast(input_dim=3, hidden_dim=hp['hidden_dim'], num_layers=hp['num_layers'],\n",
        "                             output_width=output_width, dropout=hp['dropout'])\n",
        "        result = train_model(model, train_loader, val_loader, n_epochs=hp['n_epochs'], lr=hp['lr'], device=device)\n",
        "        val_mse = result['best_val_mse']\n",
        "        if val_mse < best_val:\n",
        "            best_val = val_mse\n",
        "            best = {'hp': hp, 'model': result['model'], 'history': result['history']}\n",
        "\n",
        "    print(\"Best hyperparams:\", best['hp'], \"val_mse:\", best_val)\n",
        "    # save model\n",
        "    torch.save(best['model'].state_dict(), 'output/lstm_best.pth')\n",
        "\n",
        "    # 5) Evaluate on test set (sliding windows -> reassemble forecast to series for proper metrics)\n",
        "    # We'll create repeated forecasts for each test window and compare to true subsequent block at each step.\n",
        "    X_test = []\n",
        "    y_test_true = []\n",
        "    for x, y in test_ds:\n",
        "        X_test.append(x)\n",
        "        y_test_true.append(y)\n",
        "    X_test = np.stack(X_test)  # (num_windows, input_width, n_features)\n",
        "    y_test_true = np.stack(y_test_true)  # (num_windows, output_width, 1)\n",
        "\n",
        "    y_test_pred_scaled = predict_numpy(best['model'], X_test, device=device)  # (num_windows, output_width)\n",
        "    # inverse scale only the target column scaling\n",
        "    # scaler.scale_ and scaler.mean_ correspond to 3 features; target is column 0\n",
        "    target_std = scaler.scale_[0]\n",
        "    target_mean = scaler.mean_[0]\n",
        "    y_test_pred = y_test_pred_scaled * target_std + target_mean\n",
        "    y_test_true_unscaled = y_test_true.squeeze(-1) * target_std + target_mean\n",
        "\n",
        "    test_rmse = rmse(y_test_true_unscaled, y_test_pred)\n",
        "    test_mape = mape(y_test_true_unscaled, y_test_pred)\n",
        "    print(f\"LSTM Test RMSE: {test_rmse:.4f}, MAPE: {test_mape:.2f}%\")\n",
        "\n",
        "    # Save sample predictions to CSV\n",
        "    pd.DataFrame({\n",
        "        'y_true_flat': y_test_true_unscaled.flatten(),\n",
        "        'y_pred_flat': y_test_pred.flatten()\n",
        "    }).to_csv('output/lstm_test_preds.csv', index=False)\n",
        "\n",
        "    # 6) SARIMA baseline: fit on feat_0 training series and forecast test length\n",
        "    # For a fair comparison: forecast same number of total points as test set length *assuming one-step rolling forecast*\n",
        "    # Here we'll compute multi-step forecasts matching the test windows' first output step\n",
        "    train_series = train_df['feat_0']\n",
        "    test_len = len(test_df)  # number of time steps in test set\n",
        "    # Use seasonal_order with daily period (24) AND weekly optional; here we choose 24 seasonality.\n",
        "    sarima_pred_mean, sarima_res = sarima_forecast(train_series, test_len=test_len, order=(1,1,1), seasonal_order=(1,1,1,24))\n",
        "    # Compute RMSE / MAPE between sarima_pred_mean and true test series feat_0\n",
        "    y_true_test_series = test_df['feat_0'].values\n",
        "    sarima_rmse = rmse(y_true_test_series, sarima_pred_mean)\n",
        "    sarima_mape = mape(y_true_test_series, sarima_pred_mean)\n",
        "    print(f\"SARIMA Test RMSE: {sarima_rmse:.4f}, MAPE: {sarima_mape:.2f}%\")\n",
        "    # Save baseline\n",
        "    pd.DataFrame({'sarima_pred': sarima_pred_mean, 'y_true': y_true_test_series}).to_csv('output/sarima_preds.csv', index=False)\n",
        "\n",
        "    # 7) Explainability\n",
        "    # 7a) SHAP KernelExplainer (model-agnostic). KernelExplainer is slow; sample background\n",
        "    # We'll explain the model's predictions for the first 50 windows.\n",
        "    background_idx = np.random.choice(range(len(X_test)), size=min(50, len(X_test)), replace=False)\n",
        "    background = X_test[background_idx]  # shape (B, input_width, n_features)\n",
        "    # kernel explainer needs 2D vectorization; flatten windows\n",
        "    def model_flattened(x_flat_np):\n",
        "        # x_flat_np: (m, input_width * n_features)\n",
        "        x = x_flat_np.reshape((-1, X_test.shape[1], X_test.shape[2])).astype(np.float32)\n",
        "        with torch.no_grad():\n",
        "            t = torch.from_numpy(x).to(device)\n",
        "            pred = best['model'](t).cpu().numpy()\n",
        "        # return shape (m, output_width) -> we summarize by returning first predicted step only (or return mean)\n",
        "        return pred[:, 0, 0]  # first step prediction\n",
        "\n",
        "    print(\"Preparing SHAP KernelExplainer (this can be slow)...\")\n",
        "    shap_background_flat = background.reshape(background.shape[0], -1)\n",
        "    explainer = shap.KernelExplainer(model_flattened, shap_background_flat)\n",
        "    sample_to_explain = X_test[:20].reshape(20, -1)\n",
        "    shap_values = explainer.shap_values(sample_to_explain, nsamples=100)  # reduces runtime by using nsamples\n",
        "    # shap_values shape: (20, input_width*n_features)\n",
        "    # aggregate per feature across lag positions\n",
        "    shap_vals_arr = np.array(shap_values)\n",
        "    shap_vals_arr = shap_vals_arr.reshape(20, X_test.shape[1], X_test.shape[2])  # (20, input_width, n_features)\n",
        "    # average absolute importance across windows\n",
        "    mean_abs_shap = np.mean(np.abs(shap_vals_arr), axis=0)  # (input_width, n_features)\n",
        "\n",
        "    # Save shap importance aggregated by lag and feature\n",
        "    lag = np.arange(X_test.shape[1])\n",
        "    shap_df = pd.DataFrame(mean_abs_shap, columns=['feat_0','feat_1','feat_2'])\n",
        "    shap_df['lag'] = lag\n",
        "    shap_df.to_csv('output/shap_lag_feature_importance.csv', index=False)\n",
        "    print(\"Saved SHAP aggregated importances to output/shap_lag_feature_importance.csv\")\n",
        "\n",
        "    # 7b) Integrated Gradients via Captum â€” attribute to input window for a chosen example\n",
        "    # For captum, we provide a single example and compute attributions wrt input features across time.\n",
        "    ig = IntegratedGradients(best['model'])\n",
        "    # pick an example\n",
        "    example_idx = 5\n",
        "    x_example = torch.from_numpy(X_test[example_idx:example_idx+1].astype(np.float32)).to(device)\n",
        "    x_example.requires_grad = True\n",
        "    # baseline: zeros\n",
        "    baseline = torch.zeros_like(x_example).to(device)\n",
        "    attr, delta = ig.attribute(x_example, baselines=baseline, target=None, return_convergence_delta=True, n_steps=50)\n",
        "    # attr shape (1, input_width, n_features)\n",
        "    attr_np = attr.detach().cpu().numpy().squeeze(0)\n",
        "    pd.DataFrame(attr_np, columns=['feat_0','feat_1','feat_2']).to_csv('output/ig_attributions_example5.csv', index=False)\n",
        "    print(\"Saved Integrated Gradients attributions for example 5 to output/ig_attributions_example5.csv\")\n",
        "\n",
        "    # 8) Simple plotting for convenience (saved to files)\n",
        "    plt.figure(figsize=(10,4))\n",
        "    plt.plot(df['t'][-500:], df['feat_0'][-500:], label='feat_0 (true recent)')\n",
        "    plt.title('Recent feat_0')\n",
        "    plt.savefig('output/recent_feat0.png')\n",
        "    plt.close()\n",
        "\n",
        "    # 9) Summary metrics CSV\n",
        "    summary = {\n",
        "        'model': 'LSTM',\n",
        "        'lstm_rmse': test_rmse,\n",
        "        'lstm_mape': test_mape,\n",
        "        'sarima_rmse': sarima_rmse,\n",
        "        'sarima_mape': sarima_mape,\n",
        "        'n_samples': n,\n",
        "        'input_width': input_width,\n",
        "        'output_width': output_width\n",
        "    }\n",
        "    pd.DataFrame([summary]).to_csv('output/summary_metrics.csv', index=False)\n",
        "    print(\"Pipeline completed. Outputs in ./output/\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main_pipeline()"
      ]
    }
  ]
}